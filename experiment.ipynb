{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data analysis stage:\n",
    "#### 1. Join the data\n",
    "#### 2. Research ways to analyse how each of the features affects the probability of a transaction to be fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbdnet2520\\AppData\\Local\\Temp\\ipykernel_31928\\119424248.py:3: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv('dataset/model_dataset/transactions_train.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'IS_RECURRING_TRANSACTION' cleaned and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('dataset/model_dataset/transactions_train.csv')\n",
    "df_test = pd.read_csv('dataset/model_dataset/transactions_test.csv')\n",
    "column_name = 'IS_RECURRING_TRANSACTION'\n",
    "\n",
    "def clean_column(value):\n",
    "    if value is True:\n",
    "        return \"True\"\n",
    "    elif (value is False) or (value==\"Fals\"):\n",
    "        return \"False\"\n",
    "    return str(value)\n",
    "\n",
    "# Apply the function to the column\n",
    "df_train[column_name] = df_train[column_name].apply(clean_column)\n",
    "df_test[column_name] = df_test[column_name].apply(clean_column)\n",
    "\n",
    "df_train.to_csv('preprocessed_train.csv', index=False)\n",
    "df_test.to_csv('preprocessed_test.csv', index=False)\n",
    "\n",
    "print(f\"Column '{column_name}' cleaned and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('dataset/model_dataset/clean_transactions_train.csv')\n",
    "\n",
    "df['CUST_TO_TERM_DIST'] = np.sqrt((df['x_terminal_id'] - df['x_customer_id']) ** 2 +\n",
    "                                  (df['y_terminal__id'] - df['y_customer_id']) ** 2)\n",
    "df.drop(columns=['x_customer_id', 'y_customer_id', 'x_terminal_id', 'y_terminal__id', 'CUSTOMER_ID', 'TERMINAL_ID'], inplace=True)\n",
    "df.to_csv('cleaned_file_with_distances.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('dataset/model_dataset/transactions_train.csv')\n",
    "df_test = pd.read_csv('dataset/model_dataset/transactions_test.csv')\n",
    "column_name = 'IS_RECURRING_TRANSACTION'\n",
    "\n",
    "def clean_column(value):\n",
    "    if value is True:\n",
    "        return \"True\"\n",
    "    elif (value is False) or (value==\"Fals\"):\n",
    "        return \"False\"\n",
    "    return str(value)\n",
    "\n",
    "# Apply the function to the column\n",
    "df_train[column_name] = df_train[column_name].apply(clean_column)\n",
    "df_test[column_name] = df_test[column_name].apply(clean_column)\n",
    "\n",
    "df_train.to_csv('preprocessed_train.csv', index=False)\n",
    "df_train.to_csv('preprocessed_test.csv', index=False)\n",
    "\n",
    "print(f\"Column '{column_name}' cleaned and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TX_TS', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT', 'TX_FRAUD',\n",
       "       'TRANSACTION_GOODS_AND_SERVICES_AMOUNT', 'TRANSACTION_CASHBACK_AMOUNT',\n",
       "       'CARD_EXPIRY_DATE', 'CARD_DATA', 'CARD_BRAND', 'TRANSACTION_TYPE',\n",
       "       'TRANSACTION_STATUS', 'FAILURE_CODE', 'FAILURE_REASON',\n",
       "       'TRANSACTION_CURRENCY', 'CARD_COUNTRY_CODE', 'MERCHANT_ID',\n",
       "       'IS_RECURRING_TRANSACTION', 'ACQUIRER_ID', 'CARDHOLDER_AUTH_METHOD',\n",
       "       'x_customer_id', 'y_customer_id', 'x_terminal_id', 'y_terminal__id',\n",
       "       'BUSINESS_TYPE', 'MCC_CODE', 'LEGAL_NAME', 'FOUNDATION_DATE',\n",
       "       'TAX_EXCEMPT_INDICATOR', 'OUTLET_TYPE', 'ACTIVE_FROM', 'TRADING_FROM',\n",
       "       'ANNUAL_TURNOVER_CARD', 'ANNUAL_TURNOVER', 'AVERAGE_TICKET_SALE_AMOUNT',\n",
       "       'PAYMENT_PERCENTAGE_FACE_TO_FACE', 'PAYMENT_PERCENTAGE_ECOM',\n",
       "       'PAYMENT_PERCENTAGE_MOTO', 'DEPOSIT_REQUIRED_PERCENTAGE',\n",
       "       'DEPOSIT_PERCENTAGE', 'DELIVERY_SAME_DAYS_PERCENTAGE',\n",
       "       'DELIVERY_WEEK_ONE_PERCENTAGE', 'DELIVERY_WEEK_TWO_PERCENTAGE',\n",
       "       'DELIVERY_OVER_TWO_WEEKS_PERCENTAGE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('preprocessed_train.csv', index_col='TX_ID')\n",
    "df_test = pd.read_csv('preprocessed_test.csv', index_col='TX_ID')\n",
    "#print(df_train.columns)  # Check if the new column exists in the saved CSV\n",
    "#print(df[['CUST_TO_TERM_DIST']].head())\n",
    "#full_t_train = pd.read_csv('dataset/model_dataset/full_transactions_train.csv', index_col='TX_ID')\n",
    "#full_t_test = pd.read_csv('dataset/model_dataset/full_transactions_test.csv', index_col='TX_ID')\n",
    "#transactions_train = pd.read_csv('dataset/model_dataset/transactions_train.csv')\n",
    "#transactions_test = pd.read_csv('dataset/model_dataset/transactions_test.csv')\n",
    "customers = pd.read_csv('dataset/model_dataset/customers.csv', index_col='CUSTOMER_ID')\n",
    "merchants = pd.read_csv('dataset/model_dataset/merchants.csv', index_col='MERCHANT_ID')\n",
    "terminals = pd.read_csv('dataset/model_dataset/terminals.csv', index_col='TERMINAL_ID')\n",
    "##Merge transactions with customers on 'customer_id'\n",
    "transactions_with_customers = pd.merge(df_train, customers, on='CUSTOMER_ID', how='left')\n",
    "transactions_with_customers_terminals = pd.merge(transactions_with_customers, terminals, on='TERMINAL_ID', how='left')\n",
    "full_t = pd.merge(transactions_with_customers_terminals, merchants, on='MERCHANT_ID', how='left')\n",
    "#print(full_transactions.head())\n",
    "#full_t.to_csv('full_transactions_train.csv', index=False)\n",
    "full_t.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "cols_missing = [col for col in full_t.columns if full_t[col].isnull().any()]\n",
    "print(cols_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TX_TS', 'CUSTOMER_ID', 'TERMINAL_ID', 'TX_AMOUNT',\n",
      "       'TRANSACTION_GOODS_AND_SERVICES_AMOUNT', 'TRANSACTION_CASHBACK_AMOUNT',\n",
      "       'CARD_EXPIRY_DATE', 'CARD_DATA', 'CARD_BRAND', 'TRANSACTION_TYPE',\n",
      "       'TRANSACTION_STATUS', 'TRANSACTION_CURRENCY', 'CARD_COUNTRY_CODE',\n",
      "       'MERCHANT_ID', 'IS_RECURRING_TRANSACTION', 'x_customer_id',\n",
      "       'y_customer_id', 'x_terminal_id', 'y_terminal__id', 'BUSINESS_TYPE',\n",
      "       'MCC_CODE', 'LEGAL_NAME', 'FOUNDATION_DATE', 'TAX_EXCEMPT_INDICATOR',\n",
      "       'OUTLET_TYPE', 'ACTIVE_FROM', 'TRADING_FROM', 'ANNUAL_TURNOVER_CARD',\n",
      "       'ANNUAL_TURNOVER', 'AVERAGE_TICKET_SALE_AMOUNT',\n",
      "       'PAYMENT_PERCENTAGE_FACE_TO_FACE', 'PAYMENT_PERCENTAGE_ECOM',\n",
      "       'PAYMENT_PERCENTAGE_MOTO', 'DEPOSIT_REQUIRED_PERCENTAGE',\n",
      "       'DEPOSIT_PERCENTAGE', 'DELIVERY_SAME_DAYS_PERCENTAGE',\n",
      "       'DELIVERY_WEEK_ONE_PERCENTAGE', 'DELIVERY_WEEK_TWO_PERCENTAGE',\n",
      "       'DELIVERY_OVER_TWO_WEEKS_PERCENTAGE'],\n",
      "      dtype='object')\n",
      "Index(['TX_AMOUNT', 'TRANSACTION_GOODS_AND_SERVICES_AMOUNT',\n",
      "       'TRANSACTION_CASHBACK_AMOUNT', 'CARD_BRAND', 'TRANSACTION_TYPE',\n",
      "       'TRANSACTION_STATUS', 'TRANSACTION_CURRENCY', 'CARD_COUNTRY_CODE',\n",
      "       'IS_RECURRING_TRANSACTION', 'x_customer_id', 'y_customer_id',\n",
      "       'x_terminal_id', 'y_terminal__id', 'BUSINESS_TYPE',\n",
      "       'TAX_EXCEMPT_INDICATOR', 'OUTLET_TYPE', 'ANNUAL_TURNOVER_CARD',\n",
      "       'ANNUAL_TURNOVER', 'AVERAGE_TICKET_SALE_AMOUNT',\n",
      "       'PAYMENT_PERCENTAGE_FACE_TO_FACE', 'PAYMENT_PERCENTAGE_ECOM',\n",
      "       'PAYMENT_PERCENTAGE_MOTO', 'DEPOSIT_REQUIRED_PERCENTAGE',\n",
      "       'DEPOSIT_PERCENTAGE', 'DELIVERY_SAME_DAYS_PERCENTAGE',\n",
      "       'DELIVERY_WEEK_ONE_PERCENTAGE', 'DELIVERY_WEEK_TWO_PERCENTAGE',\n",
      "       'DELIVERY_OVER_TWO_WEEKS_PERCENTAGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "full_t.dropna(axis=0, subset=['TX_FRAUD'], inplace=True)\n",
    "y = full_t.TX_FRAUD\n",
    "full_t.drop(['TX_FRAUD'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll drop columns with missing values\n",
    "full_t.drop(cols_missing, axis=1, inplace=True)\n",
    "print(full_t.columns)\n",
    "\n",
    "drop_cols = [\n",
    "  'TX_TS',\n",
    "  'CUSTOMER_ID',\n",
    "  'TERMINAL_ID',\n",
    "  'CARD_DATA',\n",
    "  'CARD_EXPIRY_DATE',\n",
    "  'MERCHANT_ID', 'MCC_CODE',\n",
    "  'LEGAL_NAME',\n",
    "  'FOUNDATION_DATE',\n",
    "  'ACTIVE_FROM',\n",
    "  'TRADING_FROM'\n",
    "] # drop columns that cannot be encoded\n",
    "full_t.drop(drop_cols, axis=1, inplace=True)\n",
    "print(full_t.columns)\n",
    "\n",
    "# Break off validation set from training data\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(df, y, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_cols = ['TX_AMOUNT', 'TRANSACTION_GOODS_AND_SERVICES_AMOUNT','TRANSACTION_CASHBACK_AMOUNT', 'CARD_BRAND', 'TRANSACTION_TYPE',\n",
    "#          'TRANSACTION_STATUS', 'TRANSACTION_CURRENCY', 'CARD_COUNTRY_CODE','IS_RECURRING_TRANSACTION']\n",
    "#\n",
    "#X_test = df[test_cols]\n",
    "#\n",
    "#drop_X_test = X_test.select_dtypes(exclude=['object'])\n",
    "#print(drop_X_test.columns)\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "print(drop_X_train.columns)\n",
    "print(drop_X_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# ts_tx can't use ordinal/one-hot encoding\n",
    "# card_expiry_date can't use ordninal/one-hot encoding\n",
    "# merchant_id can't use ordninal/one-hot encoding\n",
    "\n",
    "cols = [\n",
    "  'CARD_BRAND',\n",
    "  'TRANSACTION_TYPE',\n",
    "  'TRANSACTION_STATUS',\n",
    "  'TRANSACTION_CURRENCY',\n",
    "  'CARD_COUNTRY_CODE',\n",
    "  'IS_RECURRING_TRANSACTION',\n",
    "\n",
    "] # ordinal encoded columns\n",
    "#enc_cols = ['CARD_BRAND', 'TRANSACTION_TYPE', 'TRANSACTION_STATUS','TRANSACTION_CURRENCY', 'CARD_COUNTRY_CODE']\n",
    "\n",
    "for col in cols:\n",
    "  print(X_test[col].unique())\n",
    "\n",
    "#for col in cols:\n",
    "#  print(X_train[col].unique())\n",
    "#  print(X_valid[col].unique())\n",
    "#  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "print(object_cols)\n",
    "print()\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if\n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "\n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(cols))\n",
    "bad_label_cols2 = list(set(good_label_cols)-set(cols))\n",
    "\n",
    "#bad = ['CARD_EXPIRY_DATE', 'FOUNDATION_DATE' ]\n",
    "\n",
    "print('Categorical columns that will be ordinal encoded:', cols)\n",
    "#print('Columns that I checked:', cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Apply ordinal encoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_train[cols] = ordinal_encoder.fit_transform(X_train[cols])\n",
    "X_valid[cols] = ordinal_encoder.transform(X_valid[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\")\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_valid)\n",
    "print(mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "OH_cols_test = pd.DataFrame(OH_encoder.fit_transform(X_test[cols]))\n",
    "#OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_test[cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_test.index = X_test.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_test = X_test.drop(object_cols, axis=1)\n",
    "#num_X_valid = X_test.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_test.columns = OH_X_test.columns.astype(str)\n",
    "\n",
    "## Use as many lines of code as you need!\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "#OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "#OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[cols]))\n",
    "#OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[cols]))\n",
    "#\n",
    "## One-hot encoding removed index; put it back\n",
    "#OH_cols_train.index = X_train.index\n",
    "#OH_cols_valid.index = X_valid.index\n",
    "#\n",
    "## Remove categorical columns (will replace with one-hot encoding)\n",
    "#num_X_train = X_train.drop(object_cols, axis=1)\n",
    "#num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "#\n",
    "## Add one-hot encoded columns to numerical features\n",
    "#OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "#OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "#\n",
    "## Ensure all columns have string type\n",
    "#OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "#OH_X_valid.columns = OH_X_valid.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from Approach 3 (One-hot Encoding):\")\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "#model.fit(OH_X_train, y_train)\n",
    "#preds = model.predict(OH_X_valid)\n",
    "#print(mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(OH_X_train, y_train)\n",
    "preds = model.predict(OH_X_valid)\n",
    "print(mean_absolute_error(y_valid, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
